---
title: "Examples of basic uses of Kcsboost package"
author: "Kipédène Coulibaly"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Examples of basic uses of Kcsboost package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Introduction
===

In almost all fields we are often confronted with data sets where the variable of interest is totally unbalanced in terms of the relative proportion of classes. For classification problems, it is widely demonstrated in the literature that this pathological situation biases the estimates if it is not treated well before. 

Several solutions have been proposed to remedy this (Menardi and Torelli 2014). Indeed, a first family of techniques known as resampling techniques is proposed and consists in manipulating the data set to rebalance it before providing it to the classifier. The second family of techniques consists of resampling using a cost matrix. Finally, the most difficult technique consists in modifying the classifier so that it takes into account the class imbalance in its design. 

Bahnsen and al ( 2014) propose an objective function that depends on a cost matrix and that allows to deal with the cost of misclassifications in the context of bank fraud detection. In 2022, Hoppner and al (2022) propose an extension of the work of Bahnsen and al (2014) but in a machine learning framework using a gradient boosting algorithm and lasso regularization. 

This version 1.0.0 of Kcsboost (K2csboost) proposes a slight modification of csboost, the algorithm proposed by Hoppner et al (2022) to take into account a matrix of size $2 \times 2$ and to deal not only with class imbalance but also with the cost of misclassification. 

Using Kcsboost package
===

The K2csboost function requires a cost matrix. The matrix must be a square matrix, taking in the first row the cost of good positive classifications (TP) and bad negative classifications (FN). In the second row we have the cost of bad positive classifications (FP) and good negative classifications.  

The package includes a bank fraud dataset that is used for this vignette.

```{r load_data}
library(Kcsboost)

## loading data 
data(creditcard)

## Creating a train set and test set that keep the same proportions of class imbalance
i0 <- which(creditcard$Class == 0)
i1 <- which(creditcard$Class == 1)

set.seed(2020)
i0_train <- sample(i0, size = 0.7 * length(i0))
i1_train <- sample(i1, size = 0.7 * length(i1))

train <- creditcard[ c(i0_train, i1_train), ]
test  <- creditcard[-c(i0_train, i1_train), ]

## Creating the cost matrix
cost_matrix_train <- cost_matrix_test <- matrix(c(25, 100, 75, 2), ncol = 2, byrow = TRUE)
```

Estimation
----

Estimation of class inbalanced model. Don't forget to remove constant :
```{r model}
K2csb<- K2csboost(formula               = Class ~ . - 1,
                   train                 = train,
                   test                  = test,
                   cost_matrix_train     = cost_matrix_train,
                   cost_matrix_test      = cost_matrix_test,
                   nrounds               = 300,
                   early_stopping_rounds = 20,
                   verbose               = 1,
                   print_every_n         = 1)

```

Summary K2csboost object :
```{r summary}
summary(K2csb)

```

Plot K2csboost object :
```{r plot}
plot(K2csb, legend_position = "right")
```

Predict K2csboost object :
```{r predict}
pred <- predict(K2csb, newdata = test)

# Confusion matrix :
# library(caret)
# confusionMatrix(factor(as.numeric(pred>0.5)),factor(test$Class))

```

Bibliography
===

BAHNSEN, A. C., AOUADA, D., OTTERSTEN, B., (2014), Example-dependent cost-sensitive logistic regression for credit scoring, IEEE, p. 263-269.  
Höppner, S., Baesens, B., Verbeke, W., and Verdonck, T. (2022). Instance-dependent cost-sensitive learning for detecting transfer fraud. European Journal of Opera- tional Research, 297(1), 291-300.  
MENARDI, G., TORELLI, N., (2014), Training and assessing classification rules with imbalanced data, Data mining and knowledge discovery, vol. 28, n°1, p. 92-122.
